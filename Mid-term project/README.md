# Bank Marketing ML Project

## Опис задачі
Цей проєкт побудований на основі `Bank Marketing Data Set` з [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing).  
Задача: передбачити, чи клієнт підпише строковий депозит (`y = yes/no`) після маркетингової кампанії.  

Основні виклики:
- Сильний класовий дисбаланс (≈ 11% підписів).
- Висока кореляція макроекономічних змінних.
- Доволі багато категоріальних ознак.

## Підхід
Було виконано наступні кроки:
1. Попередня обробка:
   - One-hot encoding для категоріальних ознак
   - Масштабування числових ознак (для логістичної регресії, kNN)
   - Інженерія ознак: `campaign_log`, `no_prev_contact`, категоризація `age`

2. Балансування класів:
   - Використано `class_weight='balanced'`
   - Додаткове тестування зміни decision threshold

3. Побудова моделей:
   - Logistic Regression
   - kNN
   - Decision Tree
   - XGBoost (бустинг, з тюнингом гіперпараметрів через RandomizedSearchCV і Hyperopt)

4. Оцінка моделей:
   - Метрика: **ROC-AUC** (оскільки важливо відрізняти класи в умовах дисбалансу)
   - Також використано Precision, Recall, F1

## Результати
| Модель                | Гіперпараметри (основні)                          | ROC-AUC train | ROC-AUC val | Коментар |
|------------------------|---------------------------------------------------|---------------|-------------|----------|
| Logistic Regression    | class_weight=balanced, L2                         | 0.935         | 0.943       | Добра базова модель, висока recall, але багато FP |
| kNN                   | n_neighbors=15                                    | 0.948         | 0.924       | Чутлива до масштабу і вибору k, слабша на val |
| Decision Tree          | max_depth=6, min_samples_leaf=50, balanced        | 0.945         | 0.945       | Інтерпретована, але менш стабільна |
| XGBoost (RandomizedCV) | n_estimators=332, lr=0.01, max_depth=7, subsample=0.72 | 0.964         | 0.955       | Найкраща модель, стабільна |
| XGBoost (Hyperopt)     | n_estimators=225, lr=0.013, max_depth=9, subsample=0.67 | 0.974         | 0.954       | Дуже схожий результат, гнучкий пошук гіперпараметрів |

## Висновки
- Найкраще показав себе XGBoost з тюнінгом (ROC-AUC ≈ 0.95).
- Логістична регресія є сильною базовою моделлю, але чутлива до дисбалансу класів.
- kNN працює гірше через високий розмірність простору.
- Decision Tree показує хороший баланс, але не перевищує бустинг.
- Аналіз SHAP підтвердив важливість макроекономічних змінних (emp.var.rate, nr.employed) і duration.
- Модель часто помиляється у травні, в макроданих модель не завжди «розуміє», чи людина зреагує на депозит, потенційне балансування класів може підвищити якість.

## Подальші кроки
- Дослідити оптимальний decision threshold для балансу Precision/Recall.
- Тестувати SMOTE та undersampling для кращої роботи з дисбалансом.
- Спробувати ансамблювання (stacking) моделей.
